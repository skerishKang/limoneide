#!/usr/bin/env python3
"""
ğŸ‹ LimoneIDE ìµœì í™”ëœ AI ì—”ì§„
ì„±ëŠ¥ ìµœì í™” ë° ìºì‹± ì‹œìŠ¤í…œì´ ì ìš©ëœ AI ì—”ì§„
"""

import os
import json
import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
from functools import lru_cache, wraps
import hashlib

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
from prometheus_client import Counter, Histogram, start_http_server

# AI ì„œë¹„ìŠ¤ ì„í¬íŠ¸
from .ai_openai import OpenAIService
from .ai_gemini import GeminiService
from .ai_claude import ClaudeService
from .ai_ollama import OllamaService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('optimized_ai_engine')

# Prometheus ë©”íŠ¸ë¦­
REQUEST_COUNT = Counter('ai_engine_requests_total', 'Total AI engine requests', ['service'])
REQUEST_DURATION = Histogram('ai_engine_request_duration_seconds', 'AI request duration', ['service'])
CACHE_HIT_COUNT = Counter('ai_engine_cache_hits_total', 'Total cache hits')
CACHE_MISS_COUNT = Counter('ai_engine_cache_misses_total', 'Total cache misses')

class PerformanceLogger:
    """ì„±ëŠ¥ ë¡œê¹… í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger('performance')
    
    def log_request(self, service: str, duration: float, success: bool, cache_hit: bool = False):
        """ìš”ì²­ ì„±ëŠ¥ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'service': service,
            'duration': duration,
            'success': success,
            'cache_hit': cache_hit
        }
        self.logger.info(json.dumps(log_data))

class CacheManager:
    """ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.cache = {}
        self.max_size = 1000
        self.ttl = 300  # 5ë¶„
    
    def _generate_key(self, prompt: str, service: str, model: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{prompt}:{service}:{model}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, prompt: str, service: str, model: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        key = self._generate_key(prompt, service, model)
        
        if key in self.cache:
            cached_data = self.cache[key]
            if time.time() - cached_data['timestamp'] < self.ttl:
                CACHE_HIT_COUNT.inc()
                return cached_data['response']
            else:
                # TTL ë§Œë£Œ
                del self.cache[key]
        
        CACHE_MISS_COUNT.inc()
        return None
    
    def set(self, prompt: str, service: str, model: str, response: str):
        """ìºì‹œì— ê°’ ì €ì¥"""
        key = self._generate_key(prompt, service, model)
        
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
        
        self.cache[key] = {
            'response': response,
            'timestamp': time.time()
        }

def measure_performance(func):
    """ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        service = kwargs.get('service', 'unknown')
        
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            REQUEST_COUNT.labels(service=service).inc()
            REQUEST_DURATION.labels(service=service).observe(duration)
            return result
        except Exception as e:
            duration = time.time() - start_time
            REQUEST_COUNT.labels(service=service).inc()
            REQUEST_DURATION.labels(service=service).observe(duration)
            raise e
    
    return wrapper

class OptimizedAIEngine:
    """
    ìµœì í™”ëœ LimoneIDE AI ì—”ì§„
    - ìºì‹± ì‹œìŠ¤í…œ
    - ë³‘ë ¬ ì²˜ë¦¬
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - ì˜¤ë¥˜ ì²˜ë¦¬
    """
    
    def __init__(self):
        """AI ì—”ì§„ ì´ˆê¸°í™”"""
        self.cache_manager = CacheManager()
        self.performance_logger = PerformanceLogger()
        
        # AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.services = {
            "openai": OpenAIService(),
            "gemini": GeminiService(),
            "claude": ClaudeService(),
            "ollama": OllamaService()
        }
        
        # ê¸°ë³¸ ì„œë¹„ìŠ¤ ì„¤ì •
        self.default_service = "gemini"  # ê°€ì¥ ë¹ ë¥¸ ì„œë¹„ìŠ¤
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
        try:
            start_http_server(8001)
            logger.info("Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘: http://localhost:8001")
        except Exception as e:
            logger.warning(f"ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    @measure_performance
    async def generate_response(self, prompt: str, service: str = None, model: str = None, use_cache: bool = True) -> str:
        """
        ìµœì í™”ëœ AI ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
            service: ì‚¬ìš©í•  AI ì„œë¹„ìŠ¤ (openai, gemini, claude, ollama)
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…
            use_cache: ìºì‹± ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            str: AI ì‘ë‹µ
        """
        if service is None:
            service = self.default_service
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached_response = self.cache_manager.get(prompt, service, model or "default")
            if cached_response:
                self.performance_logger.log_request(service, 0.0, True, cache_hit=True)
                return cached_response
        
        start_time = time.time()
        
        try:
            # AI ì„œë¹„ìŠ¤ í˜¸ì¶œ
            ai_service = self.services.get(service)
            if not ai_service:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì„œë¹„ìŠ¤: {service}")
            
            response = await ai_service.generate_response(prompt, model or "default")
            
            # ìºì‹œ ì €ì¥
            if use_cache:
                self.cache_manager.set(prompt, service, model or "default", response)
            
            duration = time.time() - start_time
            self.performance_logger.log_request(service, duration, True, cache_hit=False)
            
            return response
            
        except Exception as e:
            duration = time.time() - start_time
            self.performance_logger.log_request(service, duration, False, cache_hit=False)
            logger.error(f"AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ({service}): {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    @measure_performance
    async def generate_code(self, prompt: str, language: str = "python", service: str = None) -> str:
        """
        ìµœì í™”ëœ ì½”ë“œ ìƒì„±
        
        Args:
            prompt: ì½”ë“œ ìƒì„± ìš”ì²­
            language: í”„ë¡œê·¸ë˜ë° ì–¸ì–´
            service: ì‚¬ìš©í•  AI ì„œë¹„ìŠ¤
            
        Returns:
            str: ìƒì„±ëœ ì½”ë“œ
        """
        if service is None:
            service = self.default_service
        
        # ì½”ë“œ ìƒì„± ì „ìš© í”„ë¡¬í”„íŠ¸
        code_prompt = f"ë‹¤ìŒ ìš”ì²­ì— ë”°ë¼ {language} ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì½”ë“œë§Œ ì‘ë‹µí•˜ê³  ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:\n\n{prompt}"
        
        return await self.generate_response(code_prompt, service, use_cache=True)
    
    @measure_performance
    async def batch_generate_responses(self, prompts: List[str], service: str = None) -> List[str]:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì‘ë‹µ ìƒì„±
        
        Args:
            prompts: í”„ë¡¬í”„íŠ¸ ëª©ë¡
            service: ì‚¬ìš©í•  AI ì„œë¹„ìŠ¤
            
        Returns:
            List[str]: ì‘ë‹µ ëª©ë¡
        """
        if service is None:
            service = self.default_service
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì‹¤í–‰
        tasks = [self.generate_response(prompt, service) for prompt in prompts]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (í”„ë¡¬í”„íŠ¸ {i}): {response}")
                processed_responses.append(f"ì˜¤ë¥˜ ë°œìƒ: {str(response)}")
            else:
                processed_responses.append(response)
        
        return processed_responses
    
    async def get_available_models(self) -> Dict[str, List[str]]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
        """
        models = {}
        for service_name, service in self.services.items():
            try:
                if hasattr(service, 'get_available_models'):
                    models[service_name] = await service.get_available_models()
                else:
                    models[service_name] = ["default"]
            except Exception as e:
                logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ({service_name}): {e}")
                models[service_name] = []
        
        return models
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        ì„±ëŠ¥ í†µê³„ ë°˜í™˜
        """
        return {
            "cache_hits": CACHE_HIT_COUNT._value.get(),
            "cache_misses": CACHE_MISS_COUNT._value.get(),
            "cache_hit_rate": CACHE_HIT_COUNT._value.get() / (CACHE_HIT_COUNT._value.get() + CACHE_MISS_COUNT._value.get()) if (CACHE_HIT_COUNT._value.get() + CACHE_MISS_COUNT._value.get()) > 0 else 0,
            "total_requests": sum(REQUEST_COUNT._metrics.values()),
            "cache_size": len(self.cache_manager.cache),
            "timestamp": datetime.now().isoformat()
        }
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache_manager.cache.clear()
        logger.info("AI ì—”ì§„ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def health_check(self) -> Dict[str, Any]:
        """
        AI ì—”ì§„ ìƒíƒœ í™•ì¸
        """
        health_status = {
            "status": "healthy",
            "services": {},
            "cache": {
                "size": len(self.cache_manager.cache),
                "max_size": self.cache_manager.max_size
            },
            "performance": self.get_performance_stats(),
            "timestamp": datetime.now().isoformat()
        }
        
        # ê° ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        for service_name, service in self.services.items():
            try:
                if hasattr(service, 'is_available'):
                    health_status["services"][service_name] = {
                        "available": service.is_available(),
                        "status": "healthy" if service.is_available() else "unavailable"
                    }
                else:
                    health_status["services"][service_name] = {
                        "available": True,
                        "status": "unknown"
                    }
            except Exception as e:
                health_status["services"][service_name] = {
                    "available": False,
                    "status": f"error: {str(e)}"
                }
        
        return health_status

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
optimized_ai_engine = OptimizedAIEngine()

# í¸ì˜ í•¨ìˆ˜ë“¤
async def generate_ai_response(prompt: str, service: str = None) -> str:
    """AI ì‘ë‹µ ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    return await optimized_ai_engine.generate_response(prompt, service)

async def generate_code(prompt: str, language: str = "python", service: str = None) -> str:
    """ì½”ë“œ ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    return await optimized_ai_engine.generate_code(prompt, language, service)

async def get_ai_health() -> Dict[str, Any]:
    """AI ì—”ì§„ ìƒíƒœ í™•ì¸ (í¸ì˜ í•¨ìˆ˜)"""
    return await optimized_ai_engine.health_check() 