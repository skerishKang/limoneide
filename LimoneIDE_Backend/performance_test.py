#!/usr/bin/env python3
"""
ğŸ‹ LimoneIDE ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸
"""

import asyncio
import time
import json
import statistics
from typing import List, Dict, Any
import logging
import requests
from concurrent.futures import ThreadPoolExecutor
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('performance_test')

class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
        
    async def test_single_request(self, endpoint: str, method: str = "GET", data: Dict = None) -> Dict[str, Any]:
        """ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            if method.upper() == "GET":
                response = requests.get(f"{self.base_url}{endpoint}", timeout=30)
            elif method.upper() == "POST":
                response = requests.post(f"{self.base_url}{endpoint}", json=data, timeout=30)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” HTTP ë©”ì„œë“œ: {method}")
            
            duration = time.time() - start_time
            
            return {
                "endpoint": endpoint,
                "method": method,
                "duration": duration,
                "status_code": response.status_code,
                "success": response.status_code < 400,
                "timestamp": time.time()
            }
            
        except Exception as e:
            duration = time.time() - start_time
            return {
                "endpoint": endpoint,
                "method": method,
                "duration": duration,
                "status_code": 0,
                "success": False,
                "error": str(e),
                "timestamp": time.time()
            }
    
    async def test_endpoint_performance(self, endpoint: str, method: str = "GET", data: Dict = None, iterations: int = 10) -> Dict[str, Any]:
        """ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰)"""
        logger.info(f"ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘: {method} {endpoint}")
        
        durations = []
        success_count = 0
        error_count = 0
        
        for i in range(iterations):
            result = await self.test_single_request(endpoint, method, data)
            durations.append(result["duration"])
            
            if result["success"]:
                success_count += 1
            else:
                error_count += 1
                logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ ({i+1}/{iterations}): {result.get('error', 'Unknown error')}")
        
        # í†µê³„ ê³„ì‚°
        if durations:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "iterations": iterations,
                "success_count": success_count,
                "error_count": error_count,
                "success_rate": success_count / iterations,
                "min_duration": min(durations),
                "max_duration": max(durations),
                "avg_duration": statistics.mean(durations),
                "median_duration": statistics.median(durations),
                "std_duration": statistics.stdev(durations) if len(durations) > 1 else 0,
                "p95_duration": np.percentile(durations, 95),
                "p99_duration": np.percentile(durations, 99),
                "durations": durations
            }
        else:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "iterations": iterations,
                "success_count": 0,
                "error_count": error_count,
                "success_rate": 0,
                "error": "ëª¨ë“  ìš”ì²­ ì‹¤íŒ¨"
            }
        
        logger.info(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {endpoint} - í‰ê·  {stats.get('avg_duration', 0):.3f}ì´ˆ, ì„±ê³µë¥  {stats.get('success_rate', 0)*100:.1f}%")
        return stats
    
    async def test_concurrent_requests(self, endpoint: str, method: str = "GET", data: Dict = None, concurrent_users: int = 10) -> Dict[str, Any]:
        """ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        logger.info(f"ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ì‹œì‘: {concurrent_users}ëª…ì˜ ì‚¬ìš©ì, {method} {endpoint}")
        
        start_time = time.time()
        
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            for i in range(concurrent_users):
                future = executor.submit(asyncio.run, self.test_single_request(endpoint, method, data))
                futures.append(future)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            results = []
            for future in futures:
                try:
                    result = future.result(timeout=60)
                    results.append(result)
                except Exception as e:
                    results.append({
                        "endpoint": endpoint,
                        "method": method,
                        "duration": 60,
                        "status_code": 0,
                        "success": False,
                        "error": str(e),
                        "timestamp": time.time()
                    })
        
        total_duration = time.time() - start_time
        durations = [r["duration"] for r in results]
        success_count = sum(1 for r in results if r["success"])
        
        stats = {
            "endpoint": endpoint,
            "method": method,
            "concurrent_users": concurrent_users,
            "total_duration": total_duration,
            "success_count": success_count,
            "error_count": len(results) - success_count,
            "success_rate": success_count / len(results),
            "requests_per_second": len(results) / total_duration,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
            "avg_duration": statistics.mean(durations) if durations else 0,
            "median_duration": statistics.median(durations) if durations else 0,
            "p95_duration": np.percentile(durations, 95) if durations else 0,
            "p99_duration": np.percentile(durations, 99) if durations else 0
        }
        
        logger.info(f"ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {endpoint} - RPS {stats['requests_per_second']:.2f}, ì„±ê³µë¥  {stats['success_rate']*100:.1f}%")
        return stats
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_scenarios = [
            # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            {"endpoint": "/health", "method": "GET", "iterations": 20},
            {"endpoint": "/api/health/db", "method": "GET", "iterations": 20},
            {"endpoint": "/auth/session", "method": "GET", "iterations": 20},
            
            # API ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            {"endpoint": "/api/projects", "method": "POST", "data": {"name": "Test Project", "description": "Performance Test"}, "iterations": 10},
            {"endpoint": "/api/voice/process", "method": "POST", "data": {"command": "í…ŒìŠ¤íŠ¸ ëª…ë ¹", "language": "ko"}, "iterations": 10},
            {"endpoint": "/api/ai/generate", "method": "POST", "data": {"command": "ê°„ë‹¨í•œ ì›¹ì‚¬ì´íŠ¸ ë§Œë“¤ì–´ì¤˜", "template": "general"}, "iterations": 10},
            {"endpoint": "/api/deploy/start", "method": "POST", "data": {"project_id": "test-project", "template": "general"}, "iterations": 10},
            {"endpoint": "/api/cloudsql/test-connection", "method": "POST", "data": {"instance_name": "test-instance"}, "iterations": 10},
            {"endpoint": "/api/templates/blog", "method": "POST", "data": {"command": "ë¸”ë¡œê·¸ ë§Œë“¤ì–´ì¤˜", "features": ["blog_posts", "guestbook"]}, "iterations": 10},
            {"endpoint": "/api/guestbook", "method": "GET", "iterations": 20},
            {"endpoint": "/api/stats", "method": "GET", "iterations": 20},
        ]
        
        results = {}
        
        # ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        for scenario in test_scenarios:
            endpoint = scenario["endpoint"]
            method = scenario["method"]
            data = scenario.get("data")
            iterations = scenario["iterations"]
            
            result = await self.test_endpoint_performance(endpoint, method, data, iterations)
            results[endpoint] = result
        
        # ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ (ë¶€í•˜ í…ŒìŠ¤íŠ¸)
        load_test_scenarios = [
            {"endpoint": "/health", "method": "GET", "concurrent_users": 50},
            {"endpoint": "/api/guestbook", "method": "GET", "concurrent_users": 30},
            {"endpoint": "/api/stats", "method": "GET", "concurrent_users": 20},
        ]
        
        load_results = {}
        for scenario in load_test_scenarios:
            endpoint = scenario["endpoint"]
            method = scenario["method"]
            data = scenario.get("data")
            concurrent_users = scenario["concurrent_users"]
            
            result = await self.test_concurrent_requests(endpoint, method, data, concurrent_users)
            load_results[f"{endpoint}_load_test"] = result
        
        # ì¢…í•© ê²°ê³¼
        comprehensive_result = {
            "timestamp": time.time(),
            "test_duration": time.time() - time.time(),
            "single_request_tests": results,
            "load_tests": load_results,
            "summary": self._generate_summary(results, load_results)
        }
        
        logger.info("ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return comprehensive_result
    
    def _generate_summary(self, single_results: Dict, load_results: Dict) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        # ë‹¨ì¼ ìš”ì²­ í†µê³„
        single_durations = []
        single_success_rates = []
        
        for result in single_results.values():
            if "avg_duration" in result:
                single_durations.append(result["avg_duration"])
            if "success_rate" in result:
                single_success_rates.append(result["success_rate"])
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ í†µê³„
        load_rps = []
        load_success_rates = []
        
        for result in load_results.values():
            if "requests_per_second" in result:
                load_rps.append(result["requests_per_second"])
            if "success_rate" in result:
                load_success_rates.append(result["success_rate"])
        
        summary = {
            "single_request": {
                "avg_response_time": statistics.mean(single_durations) if single_durations else 0,
                "min_response_time": min(single_durations) if single_durations else 0,
                "max_response_time": max(single_durations) if single_durations else 0,
                "avg_success_rate": statistics.mean(single_success_rates) if single_success_rates else 0,
                "total_endpoints_tested": len(single_results)
            },
            "load_test": {
                "avg_requests_per_second": statistics.mean(load_rps) if load_rps else 0,
                "max_requests_per_second": max(load_rps) if load_rps else 0,
                "avg_success_rate": statistics.mean(load_success_rates) if load_success_rates else 0,
                "total_load_tests": len(load_results)
            },
            "performance_grade": self._calculate_performance_grade(single_durations, load_rps, single_success_rates)
        }
        
        return summary
    
    def _calculate_performance_grade(self, durations: List[float], rps: List[float], success_rates: List[float]) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        if not durations or not rps or not success_rates:
            return "F"
        
        avg_duration = statistics.mean(durations)
        avg_rps = statistics.mean(rps)
        avg_success_rate = statistics.mean(success_rates)
        
        # ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
        duration_score = max(0, 100 - (avg_duration - 1) * 50)  # 1ì´ˆ ì´í•˜: 100ì , 3ì´ˆ: 0ì 
        rps_score = min(100, avg_rps * 10)  # 10 RPS: 100ì 
        success_score = avg_success_rate * 100
        
        total_score = (duration_score * 0.4 + rps_score * 0.3 + success_score * 0.3)
        
        # ë“±ê¸‰ ê²°ì •
        if total_score >= 90:
            return "A+"
        elif total_score >= 80:
            return "A"
        elif total_score >= 70:
            return "B+"
        elif total_score >= 60:
            return "B"
        elif total_score >= 50:
            return "C+"
        elif total_score >= 40:
            return "C"
        else:
            return "F"
    
    def save_results(self, results: Dict[str, Any], filename: str = "performance_test_results.json"):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {filename}")
    
    def generate_report(self, results: Dict[str, Any], filename: str = "performance_test_report.md"):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        summary = results.get("summary", {})
        
        report = f"""# ğŸ‹ LimoneIDE ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê°œìš”
- **í…ŒìŠ¤íŠ¸ ì¼ì‹œ**: {datetime.fromtimestamp(results.get('timestamp', time.time())).strftime('%Y-%m-%d %H:%M:%S')}
- **ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„**: {results.get('test_duration', 0):.2f}ì´ˆ
- **ì„±ëŠ¥ ë“±ê¸‰**: {summary.get('performance_grade', 'N/A')}

## ğŸ“ˆ ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥
- **í‰ê·  ì‘ë‹µì‹œê°„**: {summary.get('single_request', {}).get('avg_response_time', 0):.3f}ì´ˆ
- **ìµœì†Œ ì‘ë‹µì‹œê°„**: {summary.get('single_request', {}).get('min_response_time', 0):.3f}ì´ˆ
- **ìµœëŒ€ ì‘ë‹µì‹œê°„**: {summary.get('single_request', {}).get('max_response_time', 0):.3f}ì´ˆ
- **í‰ê·  ì„±ê³µë¥ **: {summary.get('single_request', {}).get('avg_success_rate', 0)*100:.1f}%
- **í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ìˆ˜**: {summary.get('single_request', {}).get('total_endpoints_tested', 0)}ê°œ

## ğŸš€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
- **í‰ê·  ì²˜ë¦¬ëŸ‰**: {summary.get('load_test', {}).get('avg_requests_per_second', 0):.2f} RPS
- **ìµœëŒ€ ì²˜ë¦¬ëŸ‰**: {summary.get('load_test', {}).get('max_requests_per_second', 0):.2f} RPS
- **í‰ê·  ì„±ê³µë¥ **: {summary.get('load_test', {}).get('avg_success_rate', 0)*100:.1f}%
- **ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìˆ˜**: {summary.get('load_test', {}).get('total_load_tests', 0)}ê°œ

## ğŸ“‹ ìƒì„¸ ê²°ê³¼

### ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸
"""
        
        for endpoint, result in results.get("single_request_tests", {}).items():
            report += f"""
#### {endpoint}
- **ë©”ì„œë“œ**: {result.get('method', 'N/A')}
- **í‰ê·  ì‘ë‹µì‹œê°„**: {result.get('avg_duration', 0):.3f}ì´ˆ
- **ì„±ê³µë¥ **: {result.get('success_rate', 0)*100:.1f}%
- **í…ŒìŠ¤íŠ¸ íšŸìˆ˜**: {result.get('iterations', 0)}íšŒ
"""
        
        report += """
### ë¶€í•˜ í…ŒìŠ¤íŠ¸
"""
        
        for test_name, result in results.get("load_tests", {}).items():
            report += f"""
#### {test_name}
- **ë™ì‹œ ì‚¬ìš©ì**: {result.get('concurrent_users', 0)}ëª…
- **ì²˜ë¦¬ëŸ‰**: {result.get('requests_per_second', 0):.2f} RPS
- **ì„±ê³µë¥ **: {result.get('success_rate', 0)*100:.1f}%
- **í‰ê·  ì‘ë‹µì‹œê°„**: {result.get('avg_duration', 0):.3f}ì´ˆ
"""
        
        report += f"""
## ğŸ¯ ê¶Œì¥ì‚¬í•­

### ì„±ëŠ¥ ê°œì„  ìš°ì„ ìˆœìœ„
1. **ì‘ë‹µì‹œê°„ ê°œì„ **: ëª©í‘œ 1ì´ˆ ì´í•˜
2. **ì²˜ë¦¬ëŸ‰ í–¥ìƒ**: ëª©í‘œ 100 RPS ì´ìƒ
3. **ì„±ê³µë¥  ìœ ì§€**: 99% ì´ìƒ

### ìµœì í™” ë°©ì•ˆ
- ìºì‹± ì‹œìŠ¤í…œ ë„ì…
- ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”
- ë¹„ë™ê¸° ì²˜ë¦¬ ê°œì„ 
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìµœì í™”

---
**ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**í…ŒìŠ¤íŠ¸ ë„êµ¬**: LimoneIDE Performance Tester
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±: {filename}")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ‹ LimoneIDE ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì„±ëŠ¥ í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = PerformanceTester()
    
    try:
        # ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await tester.run_comprehensive_test()
        
        # ê²°ê³¼ ì €ì¥
        tester.save_results(results, "performance_test_results.json")
        tester.generate_report(results, "performance_test_report.md")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        summary = results.get("summary", {})
        logger.info("=" * 50)
        logger.info("ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ì„±ëŠ¥ ë“±ê¸‰: {summary.get('performance_grade', 'N/A')}")
        logger.info(f"í‰ê·  ì‘ë‹µì‹œê°„: {summary.get('single_request', {}).get('avg_response_time', 0):.3f}ì´ˆ")
        logger.info(f"í‰ê·  ì²˜ë¦¬ëŸ‰: {summary.get('load_test', {}).get('avg_requests_per_second', 0):.2f} RPS")
        logger.info(f"í‰ê·  ì„±ê³µë¥ : {summary.get('single_request', {}).get('avg_success_rate', 0)*100:.1f}%")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 