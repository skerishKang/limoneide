"""
ğŸ‹ LimoneIDE AI Engine
AI_Solarbotì˜ ai_handler.pyë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ì¥ëœ ë©€í‹° AI ì—”ì§„
"""

import asyncio
import json
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

# AI ëª¨ë¸ imports
import openai
import google.generativeai as genai
from anthropic import Anthropic

# ë¡œì»¬ AI (ì„ íƒì )
try:
    from ollama import Client as OllamaClient
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    model: str
    tokens_used: int
    response_time: float
    metadata: Dict[str, Any] = None

class LimoneAIEngine:
    """
    LimoneIDEì˜ í•µì‹¬ AI ì—”ì§„
    - Gemini, GPT-4, Claude, Ollama ì§€ì›
    - ìŒì„± ëª…ë ¹ ì²˜ë¦¬
    - ì›Œí¬í”Œë¡œìš° ìë™ ìƒì„±
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # AI ëª¨ë¸ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
        self.gemini_handler = GeminiHandler(self.config.get('gemini_api_key'))
        self.openai_handler = OpenAIHandler(self.config.get('openai_api_key'))
        self.claude_handler = ClaudeHandler(self.config.get('claude_api_key'))
        
        if OLLAMA_AVAILABLE:
            self.ollama_handler = OllamaHandler()
        
        # ìŒì„± ì²˜ë¦¬ ë° ì›Œí¬í”Œë¡œìš° ìƒì„±ê¸°
        self.voice_processor = VoiceProcessor()
        self.workflow_generator = WorkflowGenerator()
        
        # ì‘ë‹µ ìºì‹œ
        self.response_cache = {}
    
    async def process_voice_command(self, audio_data: bytes) -> Dict[str, Any]:
        """
        ìŒì„± â†’ ì›Œí¬í”Œë¡œìš° ìƒì„± (í•µì‹¬ ê¸°ëŠ¥)
        
        Args:
            audio_data: ìŒì„± ì˜¤ë””ì˜¤ ë°ì´í„°
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            # 1. ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜
            text = await self.voice_processor.transcribe(audio_data)
            self.logger.info(f"ìŒì„± ì¸ì‹ ê²°ê³¼: {text}")
            
            # 2. ì˜ë„ ë¶„ì„
            intent = await self.analyze_automation_intent(text)
            self.logger.info(f"ì˜ë„ ë¶„ì„: {intent}")
            
            # 3. ì›Œí¬í”Œë¡œìš° ìƒì„±
            workflow = await self.generate_workflow(intent)
            self.logger.info(f"ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
            
            # 4. ì¦‰ì‹œ ì‹¤í–‰
            result = await self.execute_workflow(workflow)
            
            return {
                "status": "success",
                "original_text": text,
                "intent": intent,
                "workflow": workflow,
                "result": result
            }
            
        except Exception as e:
            self.logger.error(f"ìŒì„± ëª…ë ¹ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def analyze_automation_intent(self, text: str) -> Dict[str, Any]:
        """
        ìì—°ì–´ â†’ ìë™í™” ì˜ë„ ë¶„ì„
        """
        prompt = f"""
        ë‹¤ìŒ ìŒì„± ëª…ë ¹ì„ ë¶„ì„í•˜ì—¬ ìë™í™” ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”:
        
        ëª…ë ¹: "{text}"
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "type": "website_creation|workflow_automation|data_analysis|content_generation",
            "target": "ì›¹ì‚¬ì´íŠ¸/ìë™í™”/ë¶„ì„/ì½˜í…ì¸  ëŒ€ìƒ",
            "features": ["í•„ìš”í•œ ê¸°ëŠ¥ë“¤"],
            "priority": "high|medium|low",
            "estimated_time": "ì˜ˆìƒ ì†Œìš” ì‹œê°„"
        }}
        """
        
        response = await self.get_best_ai_response(prompt)
        
        try:
            return json.loads(response.content)
        except json.JSONDecodeError:
            # ê¸°ë³¸ ì˜ë„ ë°˜í™˜
            return {
                "type": "general",
                "target": text,
                "features": [],
                "priority": "medium",
                "estimated_time": "5ë¶„"
            }
    
    async def generate_workflow(self, intent: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì˜ë„ â†’ ì›Œí¬í”Œë¡œìš° ìƒì„±
        """
        return await self.workflow_generator.create_workflow(intent)
    
    async def execute_workflow(self, workflow: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        """
        # ì‹¤ì œ êµ¬í˜„ì€ workflow_engine.pyì—ì„œ ì²˜ë¦¬
        return {"status": "workflow_executed", "workflow_id": workflow.get("id")}
    
    async def get_best_ai_response(self, prompt: str, model_preference: str = "auto") -> AIResponse:
        """
        ìµœì ì˜ AI ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
        """
        models = []
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ìˆ˜ì§‘
        if self.gemini_handler.is_available():
            models.append(("gemini", self.gemini_handler))
        
        if self.openai_handler.is_available():
            models.append(("gpt", self.openai_handler))
        
        if self.claude_handler.is_available():
            models.append(("claude", self.claude_handler))
        
        if not models:
            raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì„ íƒ
        if model_preference == "auto":
            # ìë™ ì„ íƒ (ë¬´ë£Œ ìš°ì„ )
            selected_model = models[0]  # Geminiê°€ ë¬´ë£Œì´ë¯€ë¡œ ìš°ì„ 
        else:
            # íŠ¹ì • ëª¨ë¸ ì„ íƒ
            selected_model = next((m for m in models if m[0] == model_preference), models[0])
        
        model_name, handler = selected_model
        
        # ì‘ë‹µ ìƒì„±
        start_time = asyncio.get_event_loop().time()
        response = await handler.generate_response(prompt)
        end_time = asyncio.get_event_loop().time()
        
        return AIResponse(
            content=response,
            model=model_name,
            tokens_used=0,  # ì‹¤ì œ í† í° ìˆ˜ëŠ” ê° í•¸ë“¤ëŸ¬ì—ì„œ ê³„ì‚°
            response_time=end_time - start_time
        )

class GeminiHandler:
    """Google Gemini AI í•¸ë“¤ëŸ¬"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key
        self.model = None
        if api_key:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-pro')
    
    def is_available(self) -> bool:
        return self.model is not None
    
    async def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            raise Exception("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        response = self.model.generate_content(prompt)
        return response.text

class OpenAIHandler:
    """OpenAI GPT í•¸ë“¤ëŸ¬"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key
        self.client = None
        if api_key:
            self.client = openai.AsyncOpenAI(api_key=api_key)
    
    def is_available(self) -> bool:
        return self.client is not None
    
    async def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

class ClaudeHandler:
    """Anthropic Claude í•¸ë“¤ëŸ¬"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key
        self.client = None
        if api_key:
            self.client = Anthropic(api_key=api_key)
    
    def is_available(self) -> bool:
        return self.client is not None
    
    async def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            raise Exception("Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        response = await self.client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text

class OllamaHandler:
    """ë¡œì»¬ Ollama í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        self.client = OllamaClient()
    
    def is_available(self) -> bool:
        try:
            # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
            return True
        except:
            return False
    
    async def generate_response(self, prompt: str) -> str:
        response = self.client.chat(model='llama2', messages=[
            {'role': 'user', 'content': prompt}
        ])
        return response['message']['content']

class VoiceProcessor:
    """ìŒì„± ì²˜ë¦¬ í´ë˜ìŠ¤ (í”„ë¡œí† íƒ€ì…)"""
    
    async def transcribe(self, audio_data: bytes) -> str:
        """
        ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ (í”„ë¡œí† íƒ€ì…)
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Whisper API ì‚¬ìš©
        """
        # ì„ì‹œ êµ¬í˜„
        return "ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ë§Œë“¤ì–´ì¤˜"

class WorkflowGenerator:
    """ì›Œí¬í”Œë¡œìš° ìƒì„±ê¸° (í”„ë¡œí† íƒ€ì…)"""
    
    async def create_workflow(self, intent: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì˜ë„ â†’ ì›Œí¬í”Œë¡œìš° ìƒì„± (í”„ë¡œí† íƒ€ì…)
        """
        workflow_id = f"workflow_{int(asyncio.get_event_loop().time())}"
        
        return {
            "id": workflow_id,
            "type": intent.get("type", "general"),
            "steps": [
                {
                    "id": "step_1",
                    "action": "analyze_requirements",
                    "input": intent.get("target", ""),
                    "output": "requirements"
                },
                {
                    "id": "step_2", 
                    "action": "generate_code",
                    "input": "requirements",
                    "output": "website_code"
                },
                {
                    "id": "step_3",
                    "action": "deploy_website",
                    "input": "website_code",
                    "output": "website_url"
                }
            ],
            "metadata": intent
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    async def test_ai_engine():
        engine = LimoneAIEngine()
        
        # ìŒì„± ëª…ë ¹ ì‹œë®¬ë ˆì´ì…˜
        test_audio = b"test_audio_data"
        result = await engine.process_voice_command(test_audio)
        print(f"ê²°ê³¼: {result}")
    
    asyncio.run(test_ai_engine()) 